{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST('data', train = True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = random_split(dataset, [50000,10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# pin_memory = True\n",
    "# print('pin_memory is', pin_memory)\n",
    " \n",
    "# for num_workers in range(0, 20, 1): \n",
    "#     tndl = torch.utils.data.DataLoader(train_ds, batch_size=128,\n",
    "#     num_workers=num_workers, pin_memory=pin_memory)\n",
    "#     start = time.time()\n",
    "#     for epoch in range(1, 5):\n",
    "#         for i, data in enumerate(train_loader):\n",
    "#             pass\n",
    "#     end = time.time()\n",
    "#     print(\"Finish with:{} second, num_workers={}\".format(end - start, num_workers))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[[2,2],[5,4]],[[2,2],[5,4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.flatten()\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3,padding=1) #out will be b,8,28,28\n",
    "        self.flatten = nn.Flatten() # out will be b,8*28*28\n",
    "        self.linear = nn.Linear(8*28*28,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x.shape\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "        \n",
    "    def training_step(self,batch):\n",
    "        images,labels = batch\n",
    "        out = model(images)\n",
    "        loss = F.cross_entropy(out,labels)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self,batch):\n",
    "        images,labels = batch\n",
    "        out = model(images)\n",
    "        loss = F.cross_entropy(out,labels)\n",
    "        acc = accuracy(out,labels)\n",
    "        return {'validation_loss':loss.detach(), 'validation_accuracy':acc}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self,model, val_loader):\n",
    "        val_out_list = [model.validation_step(batch) for batch in val_loader]\n",
    "        batch_loss = [x['validation_loss'] for x in val_out_list]\n",
    "        batch_acc = [x['validation_accuracy'] for x in val_out_list]\n",
    "        epoch_loss = torch.stack(batch_loss).mean()\n",
    "        epoch_acc = torch.stack(batch_acc).mean()\n",
    "        result = { 'val_loss':epoch_loss, 'val_acc': epoch_acc}\n",
    "        return result\n",
    "\n",
    "    def epoch_end(self,epoch, result):\n",
    "            print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "                epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "#     def validation_at_end_of_epoch(self, val_out_list):\n",
    "#         batch_loss = x[loss] for x in val_out_list\n",
    "#         batch_acc = x[acc] for x in val_out_list\n",
    "#         epoch_loss = stack(batch_loss).mean\n",
    "#         epoch_acc = stack(batch_acc).mean\n",
    "#         return { 'val_loss':epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    #def result_at_end_of_epoch():\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def evaluate(model, val_loader):\n",
    "#     val_out_list = [model.validation_step(batch) for batch in val_loader]\n",
    "#     batch_loss = [x['validation_loss'] for x in val_out_list]\n",
    "#     batch_acc = [x['validation_accuracy'] for x in val_out_list]\n",
    "#     epoch_loss = torch.stack(batch_loss).mean()\n",
    "#     epoch_acc = torch.stack(batch_acc).mean()\n",
    "#     result = { 'val_loss':epoch_loss, 'val_acc': epoch_acc}\n",
    "#     return result\n",
    "\n",
    "# def epoch_end(epoch, result):\n",
    "#         print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "#             epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,lr,model,train_loader,val_loader,opt_fn):\n",
    "    history = []\n",
    "    optimizer = opt_fn(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = model.evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "lr=0.001\n",
    "epochs=5\n",
    "opt_fn=torch.optim.SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MnistCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 1.8532, val_loss: 1.4003, val_acc: 0.7813\n",
      "Epoch [1], train_loss: 1.0682, val_loss: 0.8105, val_acc: 0.8356\n",
      "Epoch [2], train_loss: 0.6973, val_loss: 0.5966, val_acc: 0.8573\n",
      "Epoch [3], train_loss: 0.5556, val_loss: 0.5075, val_acc: 0.8717\n",
      "Epoch [4], train_loss: 0.4867, val_loss: 0.4626, val_acc: 0.8793\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': tensor(1.4003),\n",
       "  'val_acc': tensor(0.7813),\n",
       "  'train_loss': 1.8532158136367798},\n",
       " {'val_loss': tensor(0.8105),\n",
       "  'val_acc': tensor(0.8356),\n",
       "  'train_loss': 1.068240761756897},\n",
       " {'val_loss': tensor(0.5966),\n",
       "  'val_acc': tensor(0.8573),\n",
       "  'train_loss': 0.697333812713623},\n",
       " {'val_loss': tensor(0.5075),\n",
       "  'val_acc': tensor(0.8717),\n",
       "  'train_loss': 0.5555806159973145},\n",
       " {'val_loss': tensor(0.4626),\n",
       "  'val_acc': tensor(0.8793),\n",
       "  'train_loss': 0.48672884702682495}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "fit(epochs,lr,model,train_loader,val_loader,opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
