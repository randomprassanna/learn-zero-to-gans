{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST('data', train = True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = random_split(dataset, [50000,10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# pin_memory = True\n",
    "# print('pin_memory is', pin_memory)\n",
    " \n",
    "# for num_workers in range(0, 20, 1): \n",
    "#     tndl = torch.utils.data.DataLoader(train_ds, batch_size=128,\n",
    "#     num_workers=num_workers, pin_memory=pin_memory)\n",
    "#     start = time.time()\n",
    "#     for epoch in range(1, 5):\n",
    "#         for i, data in enumerate(train_loader):\n",
    "#             pass\n",
    "#     end = time.time()\n",
    "#     print(\"Finish with:{} second, num_workers={}\".format(end - start, num_workers))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[[2,2],[5,4]],[[2,2],[5,4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.flatten()\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3,padding=1) #out will be b,8,28,28\n",
    "        self.flatten = nn.Flatten() # out will be b,8*28*28\n",
    "        self.linear = nn.Linear(8*28*28,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x.shape\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "        \n",
    "    def training_step(self,batch):\n",
    "        images,labels = batch\n",
    "        out = model(images)\n",
    "        loss = F.cross_entropy(out,labels)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self,batch):\n",
    "        images,labels = batch\n",
    "        out = model(images)\n",
    "        loss = F.cross_entropy(out,labels)\n",
    "        acc = accuracy(out,labels)\n",
    "        return {'validation_loss':loss.detach(), 'validation_accuracy':acc}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, val_loader):\n",
    "        val_out_list = [self.validation_step(batch) for batch in val_loader]\n",
    "        batch_loss = [x['validation_loss'] for x in val_out_list]\n",
    "        batch_acc = [x['validation_accuracy'] for x in val_out_list]\n",
    "        epoch_loss = torch.stack(batch_loss).mean()\n",
    "        epoch_acc = torch.stack(batch_acc).mean()\n",
    "        result = { 'val_loss':epoch_loss, 'val_acc': epoch_acc}\n",
    "        return result\n",
    "\n",
    "    def epoch_end(self,epoch, result):\n",
    "            print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "                epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "#     def validation_at_end_of_epoch(self, val_out_list):\n",
    "#         batch_loss = x[loss] for x in val_out_list\n",
    "#         batch_acc = x[acc] for x in val_out_list\n",
    "#         epoch_loss = stack(batch_loss).mean\n",
    "#         epoch_acc = stack(batch_acc).mean\n",
    "#         return { 'val_loss':epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    #def result_at_end_of_epoch():\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def evaluate(model, val_loader):\n",
    "#     val_out_list = [model.validation_step(batch) for batch in val_loader]\n",
    "#     batch_loss = [x['validation_loss'] for x in val_out_list]\n",
    "#     batch_acc = [x['validation_accuracy'] for x in val_out_list]\n",
    "#     epoch_loss = torch.stack(batch_loss).mean()\n",
    "#     epoch_acc = torch.stack(batch_acc).mean()\n",
    "#     result = { 'val_loss':epoch_loss, 'val_acc': epoch_acc}\n",
    "#     return result\n",
    "\n",
    "# def epoch_end(epoch, result):\n",
    "#         print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "#             epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,lr,model,train_loader,val_loader,opt_fn):\n",
    "    history = []\n",
    "    optimizer = opt_fn(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = model.evaluate(val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "lr=0.001\n",
    "epochs=5\n",
    "opt_fn=torch.optim.SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MnistCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 2.1599, val_loss: 1.9750, val_acc: 0.6657\n",
      "Epoch [1], train_loss: 1.6696, val_loss: 1.3119, val_acc: 0.7940\n",
      "Epoch [2], train_loss: 1.0349, val_loss: 0.8127, val_acc: 0.8383\n",
      "Epoch [3], train_loss: 0.7017, val_loss: 0.6066, val_acc: 0.8616\n",
      "Epoch [4], train_loss: 0.5623, val_loss: 0.5121, val_acc: 0.8707\n",
      "Wall time: 2min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': tensor(1.9750),\n",
       "  'val_acc': tensor(0.6657),\n",
       "  'train_loss': 2.159942150115967},\n",
       " {'val_loss': tensor(1.3119),\n",
       "  'val_acc': tensor(0.7940),\n",
       "  'train_loss': 1.669646143913269},\n",
       " {'val_loss': tensor(0.8127),\n",
       "  'val_acc': tensor(0.8383),\n",
       "  'train_loss': 1.034921646118164},\n",
       " {'val_loss': tensor(0.6066),\n",
       "  'val_acc': tensor(0.8616),\n",
       "  'train_loss': 0.7016560435295105},\n",
       " {'val_loss': tensor(0.5121),\n",
       "  'val_acc': tensor(0.8707),\n",
       "  'train_loss': 0.5623199343681335}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "fit(epochs,lr,model,train_loader,val_loader,opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], train_loss: 0.4921, val_loss: 0.4665, val_acc: 0.8765\n",
      "Epoch [1], train_loss: 0.4509, val_loss: 0.4331, val_acc: 0.8847\n",
      "Epoch [2], train_loss: 0.4234, val_loss: 0.4102, val_acc: 0.8884\n",
      "Epoch [3], train_loss: 0.4039, val_loss: 0.3887, val_acc: 0.8907\n",
      "Epoch [4], train_loss: 0.3894, val_loss: 0.3755, val_acc: 0.8951\n",
      "Wall time: 2min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': tensor(0.4665),\n",
       "  'val_acc': tensor(0.8765),\n",
       "  'train_loss': 0.4920504689216614},\n",
       " {'val_loss': tensor(0.4331),\n",
       "  'val_acc': tensor(0.8847),\n",
       "  'train_loss': 0.4509265720844269},\n",
       " {'val_loss': tensor(0.4102),\n",
       "  'val_acc': tensor(0.8884),\n",
       "  'train_loss': 0.4233623147010803},\n",
       " {'val_loss': tensor(0.3887),\n",
       "  'val_acc': tensor(0.8907),\n",
       "  'train_loss': 0.4038562476634979},\n",
       " {'val_loss': tensor(0.3755),\n",
       "  'val_acc': tensor(0.8951),\n",
       "  'train_loss': 0.38939595222473145}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "fit(epochs,lr,model,train_loader,val_loader,opt_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# best results are with 0 num_workers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
