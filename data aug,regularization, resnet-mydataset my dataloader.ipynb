{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.MNIST('data', train = True) #without converting to tensor or keepi PIL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmentation and normalisation\n",
    "stats = ((0.1307 ), (0.3081)) #these will be used as mean and std dev of mnist data\n",
    "\n",
    "train_transformation = transforms.Compose([transforms.RandomCrop(28,padding=4,padding_mode='reflect'),\n",
    "                                           transforms.RandomHorizontalFlip(),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(*stats,inplace=True)])\n",
    "\n",
    "val_transformation = transforms.Compose([transforms.ToTensor(),transforms.Normalize(*stats)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = random_split(dataset, [50000,10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x20B38234D60>, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Given a dataset, creates a dataset which applies a mapping function\n",
    "    to its items (lazily, only when an item is called).\n",
    "\n",
    "    Note that data is not cloned/copied from the initial dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, map_fn):\n",
    "        self.dataset = dataset\n",
    "        self.map = map_fn\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.map(self.dataset[index])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.map:     \n",
    "            x = self.map(self.dataset[index][0]) \n",
    "        else:     \n",
    "            x = self.dataset[index][0]  # image\n",
    "        y = self.dataset[index][1]   # label      \n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms\n",
    "# import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "# class MyData(Dataset):\n",
    "#     def __init__(self):\n",
    "#         self.images = [TF.to_pil_image(x) for x in torch.ByteTensor(10, 3, 48, 48)]\n",
    "#         self.set_stage(0) # initial stage\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         image = self.images[index]\n",
    "        \n",
    "#         # Just apply your transformations here\n",
    "#         image = self.crop(image)\n",
    "#         x = TF.to_tensor(image)\n",
    "#         return x\n",
    "        \n",
    "#     def set_stage(self, stage):\n",
    "#         if stage == 0:\n",
    "#             print('Using (32, 32) crops')\n",
    "#             self.crop = transforms.RandomCrop((32, 32))\n",
    "#         elif stage == 1:\n",
    "#             print('Using (28, 28) crops')\n",
    "#             self.crop = transforms.RandomCrop((28, 28))\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "\n",
    "# dataset = MyData()\n",
    "# loader = DataLoader(dataset,\n",
    "#                     batch_size=2,\n",
    "#                     num_workers=2,\n",
    "#                     shuffle=True)\n",
    "\n",
    "# for batch_idx, data in enumerate(loader):\n",
    "#     print('Batch idx {}, data shape {}'.format(\n",
    "#         batch_idx, data.shape))\n",
    "    \n",
    "# loader.dataset.set_stage(1)\n",
    "\n",
    "# for batch_idx, data in enumerate(loader):\n",
    "#     print('Batch idx {}, data shape {}'.format(\n",
    "#         batch_idx, data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform fucntion only works with PIL images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sds = MapDataset(train_ds, train_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sds = MapDataset(val_ds, val_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image,label = train_sds[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sds[5666][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_sds[5666])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_sds, batch_size=128,  num_workers=2, pin_memory=True)\n",
    "# val_loader = DataLoader(val_sds, batch_size=128,  num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import _MultiProcessingDataLoaderIter, _BaseDataLoaderIter,_SingleProcessDataLoaderIter,_DatasetKind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDataLoader(DataLoader):\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self,dataset,batch_size, num_workers):\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_workers  = num_workers\n",
    "        super(MultiDataLoader,self).__init__(dataset, batch_size, num_workers)\n",
    "\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"o\"\"\"\n",
    "        if self.persistent_workers and self.num_workers > 0:\n",
    "            if self._iterator is None:\n",
    "                self._iterator = self._get_iterator()\n",
    "            else:\n",
    "                self._iterator._reset(self)\n",
    "            return self._iterator\n",
    "        else:\n",
    "            return self._get_iterator()\n",
    "            \n",
    "    def _get_iterator(self) -> '_BaseDataLoaderIter':\n",
    "        if self.num_workers == 0:\n",
    "            return _SingleProcessDataLoaderIter(self)\n",
    "        \n",
    "        else: \n",
    "            self.check_worker_number_rationality()\n",
    "            return _MultiProcessingDataLoaderIter(self)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        if self._dataset_kind == _DatasetKind.Iterable:\n",
    "            length = self._IterableDataset_len_called = len(self.dataset)  # type: ignore[assignment, arg-type]\n",
    "            if self.batch_size is not None:  # IterableDataset doesn't allow custom sampler or batch_sampler\n",
    "                from math import ceil\n",
    "                if self.drop_last:\n",
    "                    length = length // self.batch_size\n",
    "                else:\n",
    "                    length = ceil(length / self.batch_size)\n",
    "            return length\n",
    "        else:\n",
    "            return len(self._index_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# above custom dataloader will work by totally absent __len__ function also \n",
    "making such custome dataloader with without __len__ fn slowers the process by 3 folds with or without num_workers > 0 .\n",
    "only making custom dataset wehich is also reccomanded by docs do not slower dowwn ther process. buut ipython do not support num_worker > 0.\n",
    "try to find things in dataset class as to why such custom dataset leads to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = MultiDataLoader(train_sds, 128, num_workers=2)\n",
    "val_loader = MultiDataLoader(val_sds, 128, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for i,l in train_loader:\n",
    "    a = print(i.shape)\n",
    "    b = print(l.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels = 3, out_channels = 3, kernel_size = 3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out  = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out) + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs,labels):\n",
    "    _,preds = torch.max(outputs,dim = 1)\n",
    "    return torch.tensor((torch.sum(preds == labels).item())/len(preds))\n",
    "\n",
    "\n",
    "class ClassificationBase(nn.Module):\n",
    "    def training_step(self,batch):\n",
    "        images,labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out,labels) #it softmax done internally and labels converted internally\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch):\n",
    "        images,labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out,labels) #it softmax done internally and labels converted internally\n",
    "        acc = accuracy(out,labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc':acc}\n",
    "    \n",
    "    def evaluate(self,val_loader):\n",
    "        val_outputs = [self.validation_step(batch) for batch in val_loader]\n",
    "        batch_loss_list = [x['val_loss'] for x in val_outputs]\n",
    "        batch_acc_list = [x['val_acc'] for x in val_outputs]\n",
    "        epoch_loss = torch.stack(batch_loss_list).mean()\n",
    "        epoch_acc =torch.stack(batch_acc_list).mean()\n",
    "        result =  {'val_lossf':epoch_loss.item(), 'val_accf':epoch_acc.item()}\n",
    "        return result\n",
    "    \n",
    "    def epoch_end(self,epoch, result):\n",
    "            print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "                epoch, result['lrs'][-1], result['train_loss'], result['val_lossf'], result['val_accf']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_net(ClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 3, stride=1, padding=1) #output is 128,8,28,28\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 8, kernel_size = 3, stride=1, padding=1) #output is 128,8,28,28\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Flatten(),nn.Linear(8*28*28, 10)) #output is 128,8*28*28\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        xout  = self.conv1(x)\n",
    "        out = self.relu1(xout)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out) + xout\n",
    "        \n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cycle(epochs, max_lr, model, train_loader, val_loader, opt_fn, weight_decay=0, grad_clip=None):\n",
    "    history = []\n",
    "    \n",
    "    \n",
    "    def get_lr(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    \n",
    "    optimizer = opt_fn(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=(len(train_loader)))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "            \n",
    "            \n",
    "        # Validation phase\n",
    "        result = model.evaluate(tqdm(val_loader))\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr=0.01\n",
    "epochs= 1\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_fn= torch.optim.Adam\n",
    "\n",
    "model=Residual_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 391/391 [01:52<00:00,  3.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 79/79 [00:12<00:00,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00000, train_loss: 0.7614, val_loss: 0.4362, val_acc: 0.8556\n",
      "Wall time: 2min 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_lossf': 0.43624022603034973,\n",
       "  'val_accf': 0.855617105960846,\n",
       "  'train_loss': 0.7613905668258667,\n",
       "  'lrs': [0.0003999999999999993,\n",
       "   0.00040175115726006835,\n",
       "   0.00040700335131038136,\n",
       "   0.00041575274989354497,\n",
       "   0.0004279929690208708,\n",
       "   0.00044371507763044625,\n",
       "   0.00046290760410367705,\n",
       "   0.0004855565446355275,\n",
       "   0.0005116453734524008,\n",
       "   0.000541155054870129,\n",
       "   0.0005740640571833596,\n",
       "   0.0006103483683761249,\n",
       "   0.0006499815136421817,\n",
       "   0.0006929345747023388,\n",
       "   0.0007391762109046282,\n",
       "   0.000788672682091986,\n",
       "   0.000841387873220717,\n",
       "   0.0008972833207117979,\n",
       "   0.0009563182405157751,\n",
       "   0.001018449557870815,\n",
       "   0.0010836319387321387,\n",
       "   0.0011518178228499647,\n",
       "   0.0012229574584717903,\n",
       "   0.0012969989386436998,\n",
       "   0.0013738882390842162,\n",
       "   0.00145356925760305,\n",
       "   0.0015359838550360085,\n",
       "   0.0016210718976661584,\n",
       "   0.0017087713011003387,\n",
       "   0.0017990180755689516,\n",
       "   0.0018917463726160367,\n",
       "   0.0019868885331455147,\n",
       "   0.0020843751367885747,\n",
       "   0.002184135052556163,\n",
       "   0.002286095490739627,\n",
       "   0.0023901820560216494,\n",
       "   0.002496318801758702,\n",
       "   0.0026044282853954267,\n",
       "   0.002714431624970509,\n",
       "   0.0028262485566728146,\n",
       "   0.002939797493405778,\n",
       "   0.0030549955843173476,\n",
       "   0.003171758775252,\n",
       "   0.0032900018700807747,\n",
       "   0.0034096385928645175,\n",
       "   0.0035305816508050336,\n",
       "   0.003652742797938169,\n",
       "   0.0037760328995223797,\n",
       "   0.003900361997075802,\n",
       "   0.004025639374014339,\n",
       "   0.004151773621842935,\n",
       "   0.004278672706851656,\n",
       "   0.004406244037268003,\n",
       "   0.004534394530816375,\n",
       "   0.004663030682635455,\n",
       "   0.004792058633503927,\n",
       "   0.004921384238324742,\n",
       "   0.005050913134817984,\n",
       "   0.005180550812372203,\n",
       "   0.005310202681003972,\n",
       "   0.005439774140375356,\n",
       "   0.005569170648818952,\n",
       "   0.005698297792320095,\n",
       "   0.005827061353405948,\n",
       "   0.005955367379891165,\n",
       "   0.006083122253430001,\n",
       "   0.006210232757824826,\n",
       "   0.006336606147041226,\n",
       "   0.006462150212880028,\n",
       "   0.006586773352256922,\n",
       "   0.006710384634040533,\n",
       "   0.006832893865400229,\n",
       "   0.006954211657615209,\n",
       "   0.007074249491296885,\n",
       "   0.007192919780976949,\n",
       "   0.0073101359390140164,\n",
       "   0.007425812438772192,\n",
       "   0.007539864877025491,\n",
       "   0.007652210035542544,\n",
       "   0.0077627659418067,\n",
       "   0.007871451928827166,\n",
       "   0.007978188693997596,\n",
       "   0.008082898356959134,\n",
       "   0.008185504516425742,\n",
       "   0.008285932305930306,\n",
       "   0.008384108448450864,\n",
       "   0.00847996130987711,\n",
       "   0.00857342095127813,\n",
       "   0.008664419179933273,\n",
       "   0.008752889599088891,\n",
       "   0.008838767656404647,\n",
       "   0.008921990691054062,\n",
       "   0.009002497979444914,\n",
       "   0.009080230779526138,\n",
       "   0.0091551323736489,\n",
       "   0.009227148109950558,\n",
       "   0.009296225442231335,\n",
       "   0.009362313968294591,\n",
       "   0.009425365466722737,\n",
       "   0.009485333932061924,\n",
       "   0.009542175608389885,\n",
       "   0.009595849021242372,\n",
       "   0.009646315007874978,\n",
       "   0.009693536745838168,\n",
       "   0.00973747977984475,\n",
       "   0.009778112046910125,\n",
       "   0.00981540389974702,\n",
       "   0.009849328128397582,\n",
       "   0.009879859980087106,\n",
       "   0.009906977177284866,\n",
       "   0.009930659933958882,\n",
       "   0.009950890970012786,\n",
       "   0.009967655523894215,\n",
       "   0.00998094136336557,\n",
       "   0.00999073879442925,\n",
       "   0.009997040668400863,\n",
       "   0.009999842387125259,\n",
       "   0.009999838607939727,\n",
       "   0.009999048141297814,\n",
       "   0.009997599060647157,\n",
       "   0.009995491556901829,\n",
       "   0.009992725907722096,\n",
       "   0.009989302477477839,\n",
       "   0.009985221717200553,\n",
       "   0.009980484164523911,\n",
       "   0.00997509044361295,\n",
       "   0.009969041265081817,\n",
       "   0.009962337425900171,\n",
       "   0.009954979809288162,\n",
       "   0.00994696938460008,\n",
       "   0.009938307207196637,\n",
       "   0.009928994418305939,\n",
       "   0.009919032244873107,\n",
       "   0.009908421999398656,\n",
       "   0.009897165079765555,\n",
       "   0.00988526296905506,\n",
       "   0.009872717235351342,\n",
       "   0.00985952953153486,\n",
       "   0.009845701595064624,\n",
       "   0.009831235247749278,\n",
       "   0.009816132395507075,\n",
       "   0.009800395028114788,\n",
       "   0.009784025218945544,\n",
       "   0.00976702512469567,\n",
       "   0.009749396985100555,\n",
       "   0.009731143122639561,\n",
       "   0.009712265942230038,\n",
       "   0.009692767930910492,\n",
       "   0.00967265165751291,\n",
       "   0.009651919772324326,\n",
       "   0.009630575006737647,\n",
       "   0.009608620172891802,\n",
       "   0.009586058163301235,\n",
       "   0.009562891950474838,\n",
       "   0.009539124586524315,\n",
       "   0.00951475920276207,\n",
       "   0.009489799009288678,\n",
       "   0.009464247294569933,\n",
       "   0.009438107425003624,\n",
       "   0.009411382844476,\n",
       "   0.009384077073908053,\n",
       "   0.009356193710791636,\n",
       "   0.0093277364287155,\n",
       "   0.009298708976881313,\n",
       "   0.009269115179609698,\n",
       "   0.009238958935836386,\n",
       "   0.009208244218598545,\n",
       "   0.009176975074511333,\n",
       "   0.009145155623234761,\n",
       "   0.00911279005693094,\n",
       "   0.009079882639711767,\n",
       "   0.009046437707077134,\n",
       "   0.009012459665343735,\n",
       "   0.008977952991064542,\n",
       "   0.008942922230439023,\n",
       "   0.008907371998714187,\n",
       "   0.008871306979576537,\n",
       "   0.008834731924534997,\n",
       "   0.008797651652294914,\n",
       "   0.00876007104812319,\n",
       "   0.008721995063204672,\n",
       "   0.008683428713989829,\n",
       "   0.008644377081533846,\n",
       "   0.008604845310827204,\n",
       "   0.008564838610117833,\n",
       "   0.008524362250224937,\n",
       "   0.008483421563844573,\n",
       "   0.008442021944847075,\n",
       "   0.00840016884756641,\n",
       "   0.008357867786081604,\n",
       "   0.008315124333490237,\n",
       "   0.008271944121174225,\n",
       "   0.008228332838057883,\n",
       "   0.008184296229858414,\n",
       "   0.008139840098328925,\n",
       "   0.008094970300494058,\n",
       "   0.008049692747878331,\n",
       "   0.008004013405727307,\n",
       "   0.007957938292221677,\n",
       "   0.007911473477684384,\n",
       "   0.007864625083780863,\n",
       "   0.007817399282712512,\n",
       "   0.007769802296403535,\n",
       "   0.00772184039568119,\n",
       "   0.0076735198994496365,\n",
       "   0.007624847173857419,\n",
       "   0.007575828631458741,\n",
       "   0.007526470730368618,\n",
       "   0.007476779973412039,\n",
       "   0.007426762907267217,\n",
       "   0.007376426121603085,\n",
       "   0.007325776248211119,\n",
       "   0.007274819960131597,\n",
       "   0.007223563970774449,\n",
       "   0.007172015033034772,\n",
       "   0.007120179938403147,\n",
       "   0.00706806551607087,\n",
       "   0.007015678632030214,\n",
       "   0.006963026188169855,\n",
       "   0.006910115121365548,\n",
       "   0.006856952402566212,\n",
       "   0.006803545035875516,\n",
       "   0.006749900057629097,\n",
       "   0.0066960245354675445,\n",
       "   0.00664192556740523,\n",
       "   0.006587610280895178,\n",
       "   0.00653308583189002,\n",
       "   0.006478359403899211,\n",
       "   0.006423438207042624,\n",
       "   0.006368329477100613,\n",
       "   0.0063130404745607195,\n",
       "   0.006257578483661115,\n",
       "   0.006201950811430911,\n",
       "   0.006146164786727472,\n",
       "   0.006090227759270847,\n",
       "   0.006034147098675461,\n",
       "   0.005977930193479171,\n",
       "   0.00592158445016985,\n",
       "   0.005865117292209584,\n",
       "   0.005808536159056646,\n",
       "   0.005751848505185361,\n",
       "   0.005695061799103987,\n",
       "   0.005638183522370757,\n",
       "   0.005581221168608186,\n",
       "   0.005524182242515811,\n",
       "   0.005467074258881447,\n",
       "   0.005409904741591129,\n",
       "   0.0053526812226378576,\n",
       "   0.005295411241129265,\n",
       "   0.00523810234229435,\n",
       "   0.005180762076489409,\n",
       "   0.005123397998203285,\n",
       "   0.005066017665062078,\n",
       "   0.005008628636833444,\n",
       "   0.004951238474430592,\n",
       "   0.004893854738916169,\n",
       "   0.004836484990506083,\n",
       "   0.004779136787573462,\n",
       "   0.004721817685652856,\n",
       "   0.0046645352364447995,\n",
       "   0.0046072969868208935,\n",
       "   0.004550110477829512,\n",
       "   0.004492983243702281,\n",
       "   0.004435922810861462,\n",
       "   0.004378936696928351,\n",
       "   0.004322032409732846,\n",
       "   0.004265217446324299,\n",
       "   0.0042084992919838035,\n",
       "   0.004151885419238005,\n",
       "   0.004095383286874615,\n",
       "   0.0040390003389597345,\n",
       "   0.003982744003857093,\n",
       "   0.003926621693249389,\n",
       "   0.003870640801161806,\n",
       "   0.0038148087029878644,\n",
       "   0.0037591327545177184,\n",
       "   0.0037036202909690487,\n",
       "   0.003648278626020661,\n",
       "   0.0035931150508489145,\n",
       "   0.0035381368331671194,\n",
       "   0.0034833512162680412,\n",
       "   0.0034287654180695883,\n",
       "   0.0033743866301638795,\n",
       "   0.003320222016869751,\n",
       "   0.0032662787142888833,\n",
       "   0.003212563829365614,\n",
       "   0.003159084438950619,\n",
       "   0.003105847588868553,\n",
       "   0.0030528602929897615,\n",
       "   0.0030001295323062215,\n",
       "   0.0029476622540118053,\n",
       "   0.0028954653705870054,\n",
       "   0.002843545758888211,\n",
       "   0.0027919102592417096,\n",
       "   0.002740565674542474,\n",
       "   0.002689518769357895,\n",
       "   0.0026387762690365645,\n",
       "   0.002588344858822214,\n",
       "   0.002538231182972946,\n",
       "   0.002488441843885874,\n",
       "   0.0024389834012272502,\n",
       "   0.002389862371068251,\n",
       "   0.0023410852250264963,\n",
       "   0.0022926583894134164,\n",
       "   0.0022445882443876,\n",
       "   0.0021968811231142162,\n",
       "   0.002149543310930639,\n",
       "   0.002102581044518351,\n",
       "   0.0020560005110812755,\n",
       "   0.0020098078475306296,\n",
       "   0.0019640091396763847,\n",
       "   0.0019186104214254738,\n",
       "   0.0018736176739868441,\n",
       "   0.0018290368250834248,\n",
       "   0.0017848737481711715,\n",
       "   0.0017411342616652466,\n",
       "   0.001697824128173447,\n",
       "   0.001654949053736991,\n",
       "   0.0016125146870787648,\n",
       "   0.001570526618859099,\n",
       "   0.001528990380939216,\n",
       "   0.0014879114456524188,\n",
       "   0.0014472952250831105,\n",
       "   0.0014071470703537683,\n",
       "   0.0013674722709199367,\n",
       "   0.0013282760538733593,\n",
       "   0.0012895635832533038,\n",
       "   0.0012513399593662175,\n",
       "   0.0012136102181137714,\n",
       "   0.001176379330329381,\n",
       "   0.001139652201123311,\n",
       "   0.0011034336692364347,\n",
       "   0.00106772850640274,\n",
       "   0.0010325414167206542,\n",
       "   0.000997877036033292,\n",
       "   0.0009637399313176912,\n",
       "   0.0009301346000831172,\n",
       "   0.0008970654697785228,\n",
       "   0.0008645368972092459,\n",
       "   0.0008325531679629974,\n",
       "   0.0008011184958452498,\n",
       "   0.000770237022324069,\n",
       "   0.0007399128159844893,\n",
       "   0.0007101498719924773,\n",
       "   0.0006809521115685773,\n",
       "   0.0006523233814713004,\n",
       "   0.0006242674534903134,\n",
       "   0.0005967880239495169,\n",
       "   0.000569888713220057,\n",
       "   0.0005435730652433521,\n",
       "   0.000517844547064179,\n",
       "   0.0004927065483738951,\n",
       "   0.0004681623810638589,\n",
       "   0.0004442152787890848,\n",
       "   0.00042086839654222025,\n",
       "   0.0003981248102378765,\n",
       "   0.00037598751630738724,\n",
       "   0.0003544594313040258,\n",
       "   0.00033354339151876,\n",
       "   0.0003132421526065743,\n",
       "   0.00029355838922341475,\n",
       "   0.00027449469467380596,\n",
       "   0.000256053580569195,\n",
       "   0.00023823747649704137,\n",
       "   0.00022104872970072655,\n",
       "   0.00020448960477030924,\n",
       "   0.00018856228334416974,\n",
       "   0.00017326886382157983,\n",
       "   0.00015861136108624336,\n",
       "   0.0001445917062408397,\n",
       "   0.00013121174635260212,\n",
       "   0.0001184732442099707,\n",
       "   0.00010637787809034679,\n",
       "   9.492724153898457e-05,\n",
       "   8.412284315904218e-05,\n",
       "   7.396610641282591e-05,\n",
       "   6.445836943425258e-05,\n",
       "   5.5600884852551314e-05,\n",
       "   4.739481962723067e-05,\n",
       "   3.984125489433554e-05,\n",
       "   3.294118582400999e-05,\n",
       "   2.669552148938159e-05,\n",
       "   2.1105084746795433e-05,\n",
       "   1.6170612127402988e-05,\n",
       "   1.1892753740125645e-05,\n",
       "   8.272073186003581e-06,\n",
       "   5.3090474839426756e-06,\n",
       "   3.00406700786836e-06,\n",
       "   1.3574354352925305e-06,\n",
       "   3.693697073068151e-07,\n",
       "   4e-08]}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "fit_cycle(epochs, max_lr, model, train_loader, val_loader, grad_clip=grad_clip, weight_decay=weight_decay, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
